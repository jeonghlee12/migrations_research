{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "# Import packages to use\n",
    "import twint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Patches asyncio to allow the running of multiple event loops in Jupyter Notebooks.\n",
    "# Fixes: \"RuntimeError: This event loop is already running\"\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the distance between two lat/long coordinates\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "    return np.round(res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSantaColomaTweets(start_date, end_date):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 1\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Santa Coloma de Gramenet\\\" OR \\\"Santa Coloma\\\" OR \\\"Rambla San Sebastian\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 2\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Fluvial del Besos\\\" OR \\\"Parc Fluvial del Besos\\\" OR \\\"Fluvial del Besos\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 3\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Molinet\\\" OR \\\"Parc Molinet\\\" OR \\\"Molinet\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 4\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Plaza del Rellotge\\\" OR \\\"Pla√ßa del Rellotge\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 5\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Can Zam\\\" OR \\\"Parc Can Zam\\\" OR \\\"Can Zam\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 6\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Instituto Can Peixauet\\\" OR \\\"Institut Can Peixauet\\\" OR \\\"Can Peixauet\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 7\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Gran Sol\\\" OR \\\"Parc Gran Sol\\\" OR \\\"Gran Sol\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 8\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Escuela Tanit\\\" OR \\\"Escola Tanit\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 9\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Instituto Terra Roja\\\" OR \\\"Institut Terra Roja\\\" OR \\\"Terra Roja\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 10\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Instituto Gassol\\\" OR \\\"Institut Gassol\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 11\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"CAP Santa Rosa\\\" OR \\\"Cinto Verdaguer\\\" OR \\\"Nus de la Trinitat\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 12\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Mercado del Fondo\\\" OR \\\"Mercat del Fondo\\\" OR \\\"del Fondo\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with location near 'Santa Coloma de Gramenet': North Side\n",
    "    c = twint.Config()\n",
    "    c.Geo = \"41.46287400801948, 2.2028934732857177, 1km\"\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with location near 'Santa Coloma de Gramenet': South Side\n",
    "    c = twint.Config()\n",
    "    c.Geo = \"41.45039468429977, 2.212764002746006, 0.75km\"\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "data = getSantaColomaTweets(\"2019-07-01\", \"2019-08-02\") # effectively searches for dates 07/01 - 07/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect users posting in native language of area\n",
    "native_users = set(data.loc[data['language'].isin(['es', 'ca'])]['username'])\n",
    "pickle.dump(native_users, open( \"native_users.p\", \"wb\" ) )\n",
    "print(\"Num of native language users:\", len(native_users))\n",
    "\n",
    "# eliminate users posting in native language of area\n",
    "users_with_nonnative_lang_tweets = set(data.loc[~data['language'].isin(['es', 'ca', 'und'])]['username'])\n",
    "foreign_users = users_with_nonnative_lang_tweets - native_users\n",
    "pickle.dump(foreign_users, open( \"foreign_users.p\", \"wb\" ) )\n",
    "print(\"Num of nonnative language users:\", len(foreign_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pickle file\n",
    "native_users = pickle.load( open( \"native_users.p\", \"rb\" ) )\n",
    "foreign_users = pickle.load( open( \"foreign_users.p\", \"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    \"Santa Coloma de Gramanet\" : [x.lower() for x in [\"Santa Coloma de Gramenet\", \"Santa Coloma\"]],\n",
    "    \"Fluvial del Besos\" : [x.lower() for x in [\"Parque Fluvial del Besos\", \"Parc Fluvial del Besos\", \"Fluvial del Besos\", \"rio.*besos|besos.*rio\"]],\n",
    "    \"Parque Molinet\" : [x.lower() for x in [\"Parque Molinet\", \"Parc Molinet\", \"Molinet\"]],\n",
    "    \"Plaza del Rellotge\" : [x.lower() for x in [\"Plaza del Rellotge\", \"Pla√ßa del Rellotge\"]],\n",
    "    \"Rambla San Sebastian\" : [x.lower() for x in [\"Rambla San Sebastian\"]],\n",
    "    \"Parque Can Zam\" : [x.lower() for x in [\"Parque Can Zam\", \"Parc Can Zam\", \"Can Zam\"]],\n",
    "    \"Instituto Can Peixauet\" : [x.lower() for x in [\"Instituto Can Peixauet\", \"Institut Can Peixauet\", \"Can Peixauet\"]],\n",
    "    \"Parque Gran Sol\" : [x.lower() for x in [\"Parque Gran Sol\", \"Parc Gran Sol\", \"Gran Sol\"]],\n",
    "    \"Escuela Tanit\" : [x.lower() for x in [\"Escuela Tanit\", \"Escola Tanit\"]],\n",
    "    \"Instituto Terra Roja\" : [x.lower() for x in [\"Instituto Terra Roja\", \"Institut Terra Roja\", \"Terra Roja\"]],\n",
    "    \"Instituto Gassol\" : [x.lower() for x in [\"Instituto Gassol\", \"Institut Gassol\"]],\n",
    "    \"CAP Santa Rosa\" : [x.lower() for x in [\"CAP Santa Rosa\"]],\n",
    "    \"Cinto Verdaguer\" : [x.lower() for x in [\"Cinto Verdaguer\"]],\n",
    "    \"Mercado del Fondo\" : [x.lower() for x in [\"Mercado del Fondo\", \"Mercat del Fondo\", \"del Fondo\"]],\n",
    "    \"Nus de la Trinitat\" : [x.lower() for x in [\"Nus de la Trinitat\"]],\n",
    "    \"Macanet str\" : [x.lower() for x in [\"Ma√ßanet str\"]],\n",
    "    \"Iglesia Evangelica\" : [x.lower() for x in [\"Iglesia Evangelica\", \"Iglesia Esglesia\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreign users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign_tweets_by_location = {\n",
    "    \"Santa Coloma de Gramanet\" : pd.DataFrame(),\n",
    "    \"Fluvial del Besos\" : pd.DataFrame(),\n",
    "    \"Parque Molinet\" : pd.DataFrame(),\n",
    "    \"Plaza del Rellotge\" : pd.DataFrame(),\n",
    "    \"Rambla San Sebastian\" : pd.DataFrame(),\n",
    "    \"Parque Can Zam\" : pd.DataFrame(),\n",
    "    \"Instituto Can Peixauet\" : pd.DataFrame(),\n",
    "    \"Parque Gran Sol\" : pd.DataFrame(),\n",
    "    \"Escuela Tanit\" : pd.DataFrame(),\n",
    "    \"Instituto Terra Roja\" : pd.DataFrame(),\n",
    "    \"Instituto Gassol\" : pd.DataFrame(),\n",
    "    \"CAP Santa Rosa\" : pd.DataFrame(),\n",
    "    \"Cinto Verdaguer\" : pd.DataFrame(),\n",
    "    \"Mercado del Fondo\" : pd.DataFrame(),\n",
    "    \"Nus de la Trinitat\" : pd.DataFrame(),\n",
    "    \"Macanet str\" : pd.DataFrame(),\n",
    "    \"Iglesia Evangelica\" : pd.DataFrame()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 3766.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 18712.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 14547.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 12099.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 11430.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 9702.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 8469.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 7667.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 6999.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 8275.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 7927.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 7627.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 7279.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "start = time.time()\n",
    "\n",
    "# iterates through yearlong Tweets for the above selected users and filters users who have been associated with \n",
    "# Santa Coloma within 2 weeks \n",
    "for user in foreign_users:\n",
    "    c = twint.Config()\n",
    "    c.Username = user\n",
    "    c.Since = \"2019-01-01\"\n",
    "    c.Until = \"2019-12-31\"\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "\n",
    "    df2 = twint.storage.panda.Tweets_df\n",
    "    if (len(df2) > 0): \n",
    "        df2['tweet'] = df2['tweet'].str.lower()\n",
    "\n",
    "        for k, location_df in foreign_tweets_by_location.items():\n",
    "            pattern = '|'.join(patterns[k])\n",
    "\n",
    "            tweets = df2[df2['tweet'].str.contains(pattern)]\n",
    "            \n",
    "            location_df = location_df.append(tweets)\n",
    "            location_df = location_df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "            foreign_tweets_by_location[k] = location_df\n",
    "            \n",
    "#             df2_tweets = df2_tweets.append(tweets)\n",
    "#             df2_tweets = df2_tweets.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "\n",
    "\n",
    "#         if len(df2_tweets) > 0:\n",
    "#             max_date = datetime.strptime(max(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#             min_date = datetime.strptime(min(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#             if max_date - min_date < timedelta(days=14):\n",
    "#                 foreign_less_than_two_weeks.append(user)\n",
    "\n",
    "\n",
    "#         df2_places = df2.loc[(df2['place'] != ''), ['username','place','date']]\n",
    "#         if len(df2_places) > 0:\n",
    "#             df2_places['coordinates'] = [x['coordinates'] for x in df2_places['place']]\n",
    "#             # get distances to the two central points of Santa Coloma\n",
    "#             df2_places['dist1'] = [haversine_distance(*x, 41.45039468429977, 2.212764002746006) for x in df2_places['coordinates']]\n",
    "#             df2_places['dist2'] = [haversine_distance(*x, 41.46287400801948, 2.2028934732857177) for x in df2_places['coordinates']]\n",
    "#             df2_places = df2_places[(df2_places['dist1'] < 0.75) | (df2_places['dist2'] < 1.0)]\n",
    "\n",
    "#             if len(df2_places) > 0:\n",
    "#                 max_date = datetime.strptime(max(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#                 min_date = datetime.strptime(min(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                 if max_date - min_date < timedelta(days=14):\n",
    "#                     foreign_places.append(user)\n",
    "\n",
    "    print(str(math.floor(i * 100 / len(foreign_users))) + \"% done. Time to completion: \"\n",
    "          + str(round(((time.time() - start) / i) * (len(foreign_users) - i), 0)) + \" seconds.\")\n",
    "    i += 1\n",
    "print(\"Total time of completion: \" + str(time.time() - start) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Santa Coloma de Gramanet': 991,\n",
       " 'Fluvial del Besos': 0,\n",
       " 'Parque Molinet': 118,\n",
       " 'Plaza del Rellotge': 5,\n",
       " 'Rambla San Sebastian': 0,\n",
       " 'Parque Can Zam': 76,\n",
       " 'Instituto Can Peixauet': 7,\n",
       " 'Parque Gran Sol': 329,\n",
       " 'Escuela Tanit': 0,\n",
       " 'Instituto Terra Roja': 2,\n",
       " 'Instituto Gassol': 0,\n",
       " 'CAP Santa Rosa': 0,\n",
       " 'Cinto Verdaguer': 1,\n",
       " 'Mercado del Fondo': 2061,\n",
       " 'Nus de la Trinitat': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original freq data\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Santa Coloma de Gramanet': 597,\n",
       " 'Fluvial del Besos': 16,\n",
       " 'Parque Molinet': 107,\n",
       " 'Plaza del Rellotge': 2,\n",
       " 'Rambla San Sebastian': 0,\n",
       " 'Parque Can Zam': 32,\n",
       " 'Instituto Can Peixauet': 3,\n",
       " 'Parque Gran Sol': 44,\n",
       " 'Escuela Tanit': 0,\n",
       " 'Instituto Terra Roja': 1,\n",
       " 'Instituto Gassol': 0,\n",
       " 'CAP Santa Rosa': 0,\n",
       " 'Cinto Verdaguer': 1,\n",
       " 'Mercado del Fondo': 1777,\n",
       " 'Nus de la Trinitat': 0,\n",
       " 'Macanet str': 0,\n",
       " 'Iglesia Evangelica': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Santa Coloma de Gramanet': 135,\n",
       " 'Fluvial del Besos': 6,\n",
       " 'Parque Molinet': 41,\n",
       " 'Plaza del Rellotge': 1,\n",
       " 'Rambla San Sebastian': 0,\n",
       " 'Parque Can Zam': 7,\n",
       " 'Instituto Can Peixauet': 2,\n",
       " 'Parque Gran Sol': 37,\n",
       " 'Escuela Tanit': 0,\n",
       " 'Instituto Terra Roja': 1,\n",
       " 'Instituto Gassol': 0,\n",
       " 'CAP Santa Rosa': 0,\n",
       " 'Cinto Verdaguer': 1,\n",
       " 'Mercado del Fondo': 593,\n",
       " 'Nus de la Trinitat': 0,\n",
       " 'Macanet str': 0,\n",
       " 'Iglesia Evangelica': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(foreign_tweets_by_location, open( \"foreign_freq.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign_tweets_by_location = pickle.load( open( \"foreign_freq.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = {\n",
    "    \"Santa Coloma de Gramanet\" : pd.DataFrame(),\n",
    "    \"Fluvial del Besos\" : pd.DataFrame(),\n",
    "    \"Parque Molinet\" : pd.DataFrame(),\n",
    "    \"Plaza del Rellotge\" : pd.DataFrame(),\n",
    "    \"Rambla San Sebastian\" : pd.DataFrame(),\n",
    "    \"Parque Can Zam\" : pd.DataFrame(),\n",
    "    \"Instituto Can Peixauet\" : pd.DataFrame(),\n",
    "    \"Parque Gran Sol\" : pd.DataFrame(),\n",
    "    \"Escuela Tanit\" : pd.DataFrame(),\n",
    "    \"Instituto Terra Roja\" : pd.DataFrame(),\n",
    "    \"Instituto Gassol\" : pd.DataFrame(),\n",
    "    \"CAP Santa Rosa\" : pd.DataFrame(),\n",
    "    \"Cinto Verdaguer\" : pd.DataFrame(),\n",
    "    \"Mercado del Fondo\" : pd.DataFrame(),\n",
    "    \"Nus de la Trinitat\" : pd.DataFrame(),\n",
    "    \"Macanet str\" : pd.DataFrame(),\n",
    "    \"Iglesia Evangelica\" : pd.DataFrame()\n",
    "}\n",
    "native_users_list = list(native_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNativeUserFrequencies(user_list, tweets_by_location):\n",
    "    i = 1\n",
    "    start = time.time()\n",
    "\n",
    "    # iterates through yearlong Tweets for the above selected users and filters users who have been associated with \n",
    "    # Santa Coloma within 2 weeks \n",
    "    for user in user_list:\n",
    "        c = twint.Config()\n",
    "        c.Username = user\n",
    "        c.Since = \"2019-01-01\"\n",
    "        c.Until = \"2019-12-31\"\n",
    "        c.Pandas = True\n",
    "        c.Hide_output = True\n",
    "        twint.run.Search(c)\n",
    "\n",
    "        df2 = twint.storage.panda.Tweets_df\n",
    "        if (len(df2) > 0): \n",
    "            df2['tweet'] = df2['tweet'].str.lower()\n",
    "\n",
    "            for k, location_df in tweets_by_location.items():\n",
    "                pattern = '|'.join(patterns[k])\n",
    "\n",
    "                tweets = df2[df2['tweet'].str.contains(pattern)]\n",
    "\n",
    "#                 df2_tweets = df2_tweets.append(tweets)\n",
    "#                 df2_tweets = df2_tweets.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "                location_df = location_df.append(tweets)\n",
    "                location_df = location_df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "                tweets_by_location[k] = location_df\n",
    "\n",
    "\n",
    "#             if len(df2_tweets) > 0:\n",
    "#                 max_date = datetime.strptime(max(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#                 min_date = datetime.strptime(min(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                 if max_date - min_date < timedelta(days=14):\n",
    "#                     less_than_two_weeks.append(user)\n",
    "\n",
    "\n",
    "#             df2_places = df2.loc[(df2['place'] != ''), ['username','place','date']]\n",
    "#             if len(df2_places) > 0:\n",
    "#                 df2_places['coordinates'] = [x['coordinates'] for x in df2_places['place']]\n",
    "#                 # get distances to the two central points of Santa Coloma\n",
    "#                 df2_places['dist1'] = [haversine_distance(*x, 41.45039468429977, 2.212764002746006) for x in df2_places['coordinates']]\n",
    "#                 df2_places['dist2'] = [haversine_distance(*x, 41.46287400801948, 2.2028934732857177) for x in df2_places['coordinates']]\n",
    "#                 df2_places = df2_places[(df2_places['dist1'] < 0.75) | (df2_places['dist2'] < 1.0)]\n",
    "\n",
    "#                 if len(df2_places) > 0:\n",
    "#                     max_date = datetime.strptime(max(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#                     min_date = datetime.strptime(min(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                     if max_date - min_date < timedelta(days=14):\n",
    "#                         places.append(user)\n",
    "\n",
    "        print(str(math.floor(i * 100 / len(user_list))) + \"% done. Time to completion: \"\n",
    "              + str(round(((time.time() - start) / i) * (len(user_list) - i), 0)) + \" seconds.\")\n",
    "        i += 1\n",
    "    print(\"Total time of completion: \" + str(time.time() - start) + \" seconds.\")\n",
    "    \n",
    "    return tweets_by_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[0:1000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[1000:2000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[2000:3000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[3000:4000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[4000:5000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[5000:6000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[6000:7000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[7000:8000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[8000:9000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[9000:10000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[10000:11000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[11000:12000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[12000:13000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[13000:14000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[14000:], native_tweets_by_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Search = '\\\"Santa Coloma de Gramenet\\\" OR \\\"Santa Coloma\\\" OR \\\"Rambla San Sebastian\\\"'\n",
    "c.Since = \"2019-01-01\"\n",
    "c.Until = \"2019-01-03\"\n",
    "c.Pandas = True\n",
    "c.Hide_output = True\n",
    "twint.run.Search(c)\n",
    "\n",
    "df = twint.storage.panda.Tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
