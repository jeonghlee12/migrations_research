{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "# Import packages to use\n",
    "import twint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Patches asyncio to allow the running of multiple event loops in Jupyter Notebooks.\n",
    "# Fixes: \"RuntimeError: This event loop is already running\"\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the distance between two lat/long coordinates\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "    return np.round(res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSantaColomaTweets(start_date, end_date):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 1\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Santa Coloma de Gramenet\\\" OR \\\"Santa Coloma\\\" OR \\\"Rambla San Sebastian\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 2\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Fluvial del Besos\\\" OR \\\"Parc Fluvial del Besos\\\" OR \\\"Fluvial del Besos\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 3\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Molinet\\\" OR \\\"Parc Molinet\\\" OR \\\"Molinet\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 4\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Plaza del Rellotge\\\" OR \\\"Pla√ßa del Rellotge\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 5\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Can Zam\\\" OR \\\"Parc Can Zam\\\" OR \\\"Can Zam\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 6\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Instituto Can Peixauet\\\" OR \\\"Institut Can Peixauet\\\" OR \\\"Can Peixauet\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 7\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Parque Gran Sol\\\" OR \\\"Parc Gran Sol\\\" OR \\\"Gran Sol\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 8\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Escuela Tanit\\\" OR \\\"Escola Tanit\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 9\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Instituto Terra Roja\\\" OR \\\"Institut Terra Roja\\\" OR \\\"Terra Roja\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 10\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Instituto Gassol\\\" OR \\\"Institut Gassol\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 11\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"CAP Santa Rosa\\\" OR \\\"Cinto Verdaguer\\\" OR \\\"Nus de la Trinitat\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with the various buildings and regions near Santa Coloma - Part 12\n",
    "    c = twint.Config()\n",
    "    c.Search = '\\\"Mercado del Fondo\\\" OR \\\"Mercat del Fondo\\\" OR \\\"del Fondo\\\"'\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with location near 'Santa Coloma de Gramenet': North Side\n",
    "    c = twint.Config()\n",
    "    c.Geo = \"41.46287400801948, 2.2028934732857177, 1km\"\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    # Search Tweets with location near 'Santa Coloma de Gramenet': South Side\n",
    "    c = twint.Config()\n",
    "    c.Geo = \"41.45039468429977, 2.212764002746006, 0.75km\"\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = df.append(twint.storage.panda.Tweets_df)\n",
    "    df = df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "data = getSantaColomaTweets(\"2019-07-01\", \"2019-08-02\") # effectively searches for dates 07/01 - 07/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of native language users: 14570\n",
      "Num of nonnative language users: 808\n"
     ]
    }
   ],
   "source": [
    "# collect users posting in native language of area\n",
    "native_users = set(data.loc[data['language'].isin(['es', 'ca'])]['username'])\n",
    "pickle.dump(native_users, open( \"native_users.p\", \"wb\" ) )\n",
    "print(\"Num of native language users:\", len(native_users))\n",
    "\n",
    "# eliminate users posting in native language of area\n",
    "users_with_nonnative_lang_tweets = set(data.loc[~data['language'].isin(['es', 'ca', 'und'])]['username'])\n",
    "foreign_users = users_with_nonnative_lang_tweets - native_users\n",
    "pickle.dump(foreign_users, open( \"foreign_users.p\", \"wb\" ) )\n",
    "print(\"Num of nonnative language users:\", len(foreign_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pickle file\n",
    "native_users = pickle.load( open( \"native_users.p\", \"rb\" ) )\n",
    "foreign_users = pickle.load( open( \"foreign_users.p\", \"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    \"Santa Coloma de Gramanet\" : [x.lower() for x in [\"Santa Coloma de Gramenet\", \"Santa Coloma\"]],\n",
    "    \"Fluvial del Besos\" : [x.lower() for x in [\"Parque Fluvial del Besos\", \"Parc Fluvial del Besos\", \"Fluvial del Besos\", \"rio.*besos|besos.*rio\"]],\n",
    "    \"Parque Molinet\" : [x.lower() for x in [\"Parque Molinet\", \"Parc Molinet\", \"Molinet\"]],\n",
    "    \"Plaza del Rellotge\" : [x.lower() for x in [\"Plaza del Rellotge\", \"Pla√ßa del Rellotge\"]],\n",
    "    \"Rambla San Sebastian\" : [x.lower() for x in [\"Rambla San Sebastian\"]],\n",
    "    \"Parque Can Zam\" : [x.lower() for x in [\"Parque Can Zam\", \"Parc Can Zam\", \"Can Zam\"]],\n",
    "    \"Instituto Can Peixauet\" : [x.lower() for x in [\"Instituto Can Peixauet\", \"Institut Can Peixauet\", \"Can Peixauet\"]],\n",
    "    \"Parque Gran Sol\" : [x.lower() for x in [\"Parque Gran Sol\", \"Parc Gran Sol\", \"Gran Sol\"]],\n",
    "    \"Escuela Tanit\" : [x.lower() for x in [\"Escuela Tanit\", \"Escola Tanit\"]],\n",
    "    \"Instituto Terra Roja\" : [x.lower() for x in [\"Instituto Terra Roja\", \"Institut Terra Roja\", \"Terra Roja\"]],\n",
    "    \"Instituto Gassol\" : [x.lower() for x in [\"Instituto Gassol\", \"Institut Gassol\"]],\n",
    "    \"CAP Santa Rosa\" : [x.lower() for x in [\"CAP Santa Rosa\"]],\n",
    "    \"Cinto Verdaguer\" : [x.lower() for x in [\"Cinto Verdaguer\"]],\n",
    "    \"Mercado del Fondo\" : [x.lower() for x in [\"Mercado del Fondo\", \"Mercat del Fondo\", \"del Fondo\"]],\n",
    "    \"Nus de la Trinitat\" : [x.lower() for x in [\"Nus de la Trinitat\"]],\n",
    "    \"Macanet str\" : [x.lower() for x in [\"Ma√ßanet str\"]],\n",
    "    \"Iglesia Evangelica\" : [x.lower() for x in [\"Iglesia Evangelica\", \"Iglesia Esglesia\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreign users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign_tweets_by_location = {\n",
    "    \"Santa Coloma de Gramanet\" : pd.DataFrame(),\n",
    "    \"Fluvial del Besos\" : pd.DataFrame(),\n",
    "    \"Parque Molinet\" : pd.DataFrame(),\n",
    "    \"Plaza del Rellotge\" : pd.DataFrame(),\n",
    "    \"Rambla San Sebastian\" : pd.DataFrame(),\n",
    "    \"Parque Can Zam\" : pd.DataFrame(),\n",
    "    \"Instituto Can Peixauet\" : pd.DataFrame(),\n",
    "    \"Parque Gran Sol\" : pd.DataFrame(),\n",
    "    \"Escuela Tanit\" : pd.DataFrame(),\n",
    "    \"Instituto Terra Roja\" : pd.DataFrame(),\n",
    "    \"Instituto Gassol\" : pd.DataFrame(),\n",
    "    \"CAP Santa Rosa\" : pd.DataFrame(),\n",
    "    \"Cinto Verdaguer\" : pd.DataFrame(),\n",
    "    \"Mercado del Fondo\" : pd.DataFrame(),\n",
    "    \"Nus de la Trinitat\" : pd.DataFrame(),\n",
    "    \"Macanet str\" : pd.DataFrame(),\n",
    "    \"Iglesia Evangelica\" : pd.DataFrame()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 3766.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 18712.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 14547.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 12099.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 11430.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 9702.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 8469.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "0% done. Time to completion: 7667.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 6999.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 8275.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 7927.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 7627.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 7279.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 11093.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 12752.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1% done. Time to completion: 13395.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 12650.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 13039.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 14270.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 13631.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 33955.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 36215.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 39376.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2% done. Time to completion: 38145.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 37266.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 36472.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 47334.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 45618.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 44174.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 43800.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 42449.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3% done. Time to completion: 41462.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 40681.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 39560.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 38571.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 37774.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 37299.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 36474.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 35664.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4% done. Time to completion: 34956.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 34340.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 34805.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 34112.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 33902.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 36535.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 36116.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 35584.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5% done. Time to completion: 35118.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 35450.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 35622.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 34895.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 34222.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 33604.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 33314.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 32795.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6% done. Time to completion: 32204.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 32014.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 31975.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 31584.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 35726.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 38588.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 38153.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 37740.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7% done. Time to completion: 38032.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 37415.0 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 36969.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 44614.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 44023.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 43402.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 42750.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 42298.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8% done. Time to completion: 41681.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 41516.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 41083.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 40503.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 40827.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 40410.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 42211.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 51348.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9% done. Time to completion: 50660.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 49999.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 49848.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 49263.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 48636.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 48036.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 47554.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 47042.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10% done. Time to completion: 46480.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 46044.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 47318.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 46832.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 46332.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 46487.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 46313.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 45789.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11% done. Time to completion: 46468.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 46263.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 45812.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 47390.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 46892.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 46713.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 46352.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 46011.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 45529.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12% done. Time to completion: 45051.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 45001.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 44535.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 44089.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 43647.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 43214.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 42996.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 42582.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "13% done. Time to completion: 42190.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 41769.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 41717.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 41546.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 41170.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 40788.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 40427.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 40065.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14% done. Time to completion: 39696.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 39323.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 39001.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 38639.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 38541.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 38302.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 37968.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 37752.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15% done. Time to completion: 37422.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 37133.0 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 36813.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 36577.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 36837.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 36570.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 36377.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 36063.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "16% done. Time to completion: 35833.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 35568.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 35296.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 35187.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 35010.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 34791.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 34604.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 34365.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17% done. Time to completion: 36014.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 35723.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 48240.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 47859.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 48901.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 49125.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 48734.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 48347.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "18% done. Time to completion: 48204.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "19% done. Time to completion: 47825.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "19% done. Time to completion: 47448.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "19% done. Time to completion: 48779.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "19% done. Time to completion: 48830.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "19% done. Time to completion: 48452.0 seconds.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "19% done. Time to completion: 48083.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'user'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot find twitter account with name = spadmilan1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-8b73af24820a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHide_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtwint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpanda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweets_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\twint\\run.py\u001b[0m in \u001b[0;36mSearch\u001b[1;34m(config, callback)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFollowers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPandas_au\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpanda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autoget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tweet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\twint\\run.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(config, callback)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTwint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[0;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_thread_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\asyncio\\futures.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\twint\\run.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mawait\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\asyncio\\futures.py\u001b[0m in \u001b[0;36m__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[1;31m# This tells Task to wait for completion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"await wasn't used with future\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m             \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;31m# This may also be a cancellation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\asyncio\\futures.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    247\u001b[0m                 \u001b[1;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\twint\\run.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUser_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUsername\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUser_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot find twitter account with name = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUsername\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;31m# TODO : will need to modify it to work with the new endpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot find twitter account with name = spadmilan1"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "start = time.time()\n",
    "\n",
    "# iterates through yearlong Tweets for the above selected users and filters users who have been associated with \n",
    "# Santa Coloma within 2 weeks \n",
    "for user in foreign_users:\n",
    "    c = twint.Config()\n",
    "    c.Username = user\n",
    "    c.Since = \"2019-01-01\"\n",
    "    c.Until = \"2019-12-31\"\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    twint.run.Search(c)\n",
    "\n",
    "    df2 = twint.storage.panda.Tweets_df\n",
    "    if (len(df2) > 0): \n",
    "        df2['tweet'] = df2['tweet'].str.lower()\n",
    "\n",
    "        for k, location_df in foreign_tweets_by_location.items():\n",
    "            pattern = '|'.join(patterns[k])\n",
    "\n",
    "            tweets = df2[df2['tweet'].str.contains(pattern)]\n",
    "            \n",
    "            location_df = location_df.append(tweets)\n",
    "            location_df = location_df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "            foreign_tweets_by_location[k] = location_df\n",
    "            \n",
    "#             df2_tweets = df2_tweets.append(tweets)\n",
    "#             df2_tweets = df2_tweets.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "\n",
    "\n",
    "#         if len(df2_tweets) > 0:\n",
    "#             max_date = datetime.strptime(max(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#             min_date = datetime.strptime(min(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#             if max_date - min_date < timedelta(days=14):\n",
    "#                 foreign_less_than_two_weeks.append(user)\n",
    "\n",
    "\n",
    "#         df2_places = df2.loc[(df2['place'] != ''), ['username','place','date']]\n",
    "#         if len(df2_places) > 0:\n",
    "#             df2_places['coordinates'] = [x['coordinates'] for x in df2_places['place']]\n",
    "#             # get distances to the two central points of Santa Coloma\n",
    "#             df2_places['dist1'] = [haversine_distance(*x, 41.45039468429977, 2.212764002746006) for x in df2_places['coordinates']]\n",
    "#             df2_places['dist2'] = [haversine_distance(*x, 41.46287400801948, 2.2028934732857177) for x in df2_places['coordinates']]\n",
    "#             df2_places = df2_places[(df2_places['dist1'] < 0.75) | (df2_places['dist2'] < 1.0)]\n",
    "\n",
    "#             if len(df2_places) > 0:\n",
    "#                 max_date = datetime.strptime(max(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#                 min_date = datetime.strptime(min(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                 if max_date - min_date < timedelta(days=14):\n",
    "#                     foreign_places.append(user)\n",
    "\n",
    "    print(str(math.floor(i * 100 / len(foreign_users))) + \"% done. Time to completion: \"\n",
    "          + str(round(((time.time() - start) / i) * (len(foreign_users) - i), 0)) + \" seconds.\")\n",
    "    i += 1\n",
    "print(\"Total time of completion: \" + str(time.time() - start) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Santa Coloma de Gramanet': 991,\n",
       " 'Fluvial del Besos': 0,\n",
       " 'Parque Molinet': 118,\n",
       " 'Plaza del Rellotge': 5,\n",
       " 'Rambla San Sebastian': 0,\n",
       " 'Parque Can Zam': 76,\n",
       " 'Instituto Can Peixauet': 7,\n",
       " 'Parque Gran Sol': 329,\n",
       " 'Escuela Tanit': 0,\n",
       " 'Instituto Terra Roja': 2,\n",
       " 'Instituto Gassol': 0,\n",
       " 'CAP Santa Rosa': 0,\n",
       " 'Cinto Verdaguer': 1,\n",
       " 'Mercado del Fondo': 2061,\n",
       " 'Nus de la Trinitat': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original freq data\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Santa Coloma de Gramanet': 597,\n",
       " 'Fluvial del Besos': 16,\n",
       " 'Parque Molinet': 107,\n",
       " 'Plaza del Rellotge': 2,\n",
       " 'Rambla San Sebastian': 0,\n",
       " 'Parque Can Zam': 32,\n",
       " 'Instituto Can Peixauet': 3,\n",
       " 'Parque Gran Sol': 44,\n",
       " 'Escuela Tanit': 0,\n",
       " 'Instituto Terra Roja': 1,\n",
       " 'Instituto Gassol': 0,\n",
       " 'CAP Santa Rosa': 0,\n",
       " 'Cinto Verdaguer': 1,\n",
       " 'Mercado del Fondo': 1777,\n",
       " 'Nus de la Trinitat': 0,\n",
       " 'Macanet str': 0,\n",
       " 'Iglesia Evangelica': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Santa Coloma de Gramanet': 135,\n",
       " 'Fluvial del Besos': 6,\n",
       " 'Parque Molinet': 41,\n",
       " 'Plaza del Rellotge': 1,\n",
       " 'Rambla San Sebastian': 0,\n",
       " 'Parque Can Zam': 7,\n",
       " 'Instituto Can Peixauet': 2,\n",
       " 'Parque Gran Sol': 37,\n",
       " 'Escuela Tanit': 0,\n",
       " 'Instituto Terra Roja': 1,\n",
       " 'Instituto Gassol': 0,\n",
       " 'CAP Santa Rosa': 0,\n",
       " 'Cinto Verdaguer': 1,\n",
       " 'Mercado del Fondo': 593,\n",
       " 'Nus de la Trinitat': 0,\n",
       " 'Macanet str': 0,\n",
       " 'Iglesia Evangelica': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(foreign_tweets_by_location, open( \"foreign_freq.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign_tweets_by_location = pickle.load( open( \"foreign_freq.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = {\n",
    "    \"Santa Coloma de Gramanet\" : pd.DataFrame(),\n",
    "    \"Fluvial del Besos\" : pd.DataFrame(),\n",
    "    \"Parque Molinet\" : pd.DataFrame(),\n",
    "    \"Plaza del Rellotge\" : pd.DataFrame(),\n",
    "    \"Rambla San Sebastian\" : pd.DataFrame(),\n",
    "    \"Parque Can Zam\" : pd.DataFrame(),\n",
    "    \"Instituto Can Peixauet\" : pd.DataFrame(),\n",
    "    \"Parque Gran Sol\" : pd.DataFrame(),\n",
    "    \"Escuela Tanit\" : pd.DataFrame(),\n",
    "    \"Instituto Terra Roja\" : pd.DataFrame(),\n",
    "    \"Instituto Gassol\" : pd.DataFrame(),\n",
    "    \"CAP Santa Rosa\" : pd.DataFrame(),\n",
    "    \"Cinto Verdaguer\" : pd.DataFrame(),\n",
    "    \"Mercado del Fondo\" : pd.DataFrame(),\n",
    "    \"Nus de la Trinitat\" : pd.DataFrame(),\n",
    "    \"Macanet str\" : pd.DataFrame(),\n",
    "    \"Iglesia Evangelica\" : pd.DataFrame()\n",
    "}\n",
    "native_users_list = list(native_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNativeUserFrequencies(user_list, tweets_by_location):\n",
    "    i = 1\n",
    "    start = time.time()\n",
    "\n",
    "    # iterates through yearlong Tweets for the above selected users and filters users who have been associated with \n",
    "    # Santa Coloma within 2 weeks \n",
    "    for user in user_list:\n",
    "        c = twint.Config()\n",
    "        c.Username = user\n",
    "        c.Since = \"2019-01-01\"\n",
    "        c.Until = \"2019-12-31\"\n",
    "        c.Pandas = True\n",
    "        c.Hide_output = True\n",
    "        twint.run.Search(c)\n",
    "\n",
    "        df2 = twint.storage.panda.Tweets_df\n",
    "        if (len(df2) > 0): \n",
    "            df2['tweet'] = df2['tweet'].str.lower()\n",
    "\n",
    "            for k, location_df in tweets_by_location.items():\n",
    "                pattern = '|'.join(patterns[k])\n",
    "\n",
    "                tweets = df2[df2['tweet'].str.contains(pattern)]\n",
    "\n",
    "#                 df2_tweets = df2_tweets.append(tweets)\n",
    "#                 df2_tweets = df2_tweets.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "                location_df = location_df.append(tweets)\n",
    "                location_df = location_df.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "                tweets_by_location[k] = location_df\n",
    "\n",
    "\n",
    "#             if len(df2_tweets) > 0:\n",
    "#                 max_date = datetime.strptime(max(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#                 min_date = datetime.strptime(min(df2_tweets['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                 if max_date - min_date < timedelta(days=14):\n",
    "#                     less_than_two_weeks.append(user)\n",
    "\n",
    "\n",
    "#             df2_places = df2.loc[(df2['place'] != ''), ['username','place','date']]\n",
    "#             if len(df2_places) > 0:\n",
    "#                 df2_places['coordinates'] = [x['coordinates'] for x in df2_places['place']]\n",
    "#                 # get distances to the two central points of Santa Coloma\n",
    "#                 df2_places['dist1'] = [haversine_distance(*x, 41.45039468429977, 2.212764002746006) for x in df2_places['coordinates']]\n",
    "#                 df2_places['dist2'] = [haversine_distance(*x, 41.46287400801948, 2.2028934732857177) for x in df2_places['coordinates']]\n",
    "#                 df2_places = df2_places[(df2_places['dist1'] < 0.75) | (df2_places['dist2'] < 1.0)]\n",
    "\n",
    "#                 if len(df2_places) > 0:\n",
    "#                     max_date = datetime.strptime(max(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "#                     min_date = datetime.strptime(min(df2_places['date']), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                     if max_date - min_date < timedelta(days=14):\n",
    "#                         places.append(user)\n",
    "\n",
    "        print(str(math.floor(i * 100 / len(user_list))) + \"% done. Time to completion: \"\n",
    "              + str(round(((time.time() - start) / i) * (len(user_list) - i), 0)) + \" seconds.\")\n",
    "        i += 1\n",
    "    print(\"Total time of completion: \" + str(time.time() - start) + \" seconds.\")\n",
    "    \n",
    "    return tweets_by_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[0:1000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[1000:2000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[2000:3000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[3000:4000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[4000:5000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[5000:6000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[6000:7000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[7000:8000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[8000:9000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[9000:10000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[10000:11000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[11000:12000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[12000:13000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[13000:14000], native_tweets_by_location)\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_tweets_by_location = getNativeUserFrequencies(native_users_list[14000:], native_tweets_by_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Search = '\\\"Santa Coloma de Gramenet\\\" OR \\\"Santa Coloma\\\" OR \\\"Rambla San Sebastian\\\"'\n",
    "c.Since = \"2019-01-01\"\n",
    "c.Until = \"2019-01-03\"\n",
    "c.Pandas = True\n",
    "c.Hide_output = True\n",
    "twint.run.Search(c)\n",
    "\n",
    "df = twint.storage.panda.Tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
